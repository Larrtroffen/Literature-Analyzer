"""
æ–‡çŒ®åˆ†æžåº”ç”¨ä¸»ç¨‹åº

åŸºäºŽStreamlitçš„äº¤äº’å¼æ–‡çŒ®åˆ†æžåº”ç”¨ï¼Œä¸“é—¨å¤„ç†Web of Scienceå¯¼å‡ºçš„Excelæ–‡ä»¶ã€‚
æä¾›å®Œæ•´çš„æ•°æ®å¤„ç†ã€NLPåˆ†æžå’Œå¯è§†åŒ–åŠŸèƒ½ã€‚
"""

import logging
import subprocess
import sys
import os
from typing import List, Optional, Dict, Any
import pandas as pd
import streamlit as st
from streamlit import cache_data, cache_resource
import numpy as np

import subprocess
import sys
import spacy
import os

# å®šä¹‰è¦ä¸‹è½½çš„spaCyæ¨¡åž‹åç§°
MODEL_NAME = "en_core_web_sm"

def download_spacy_model(model_name):
    """
    å°è¯•åŠ è½½spaCyæ¨¡åž‹ï¼Œå¦‚æžœä¸å­˜åœ¨åˆ™å°è¯•ä¸‹è½½ã€‚
    """
    try:
        # å°è¯•åŠ è½½æ¨¡åž‹
        nlp = spacy.load(model_name)
        print(f"âœ… spaCy æ¨¡åž‹ '{model_name}' å·²åŠ è½½ã€‚")
        return nlp
    except OSError:
        # å¦‚æžœæ¨¡åž‹ä¸å­˜åœ¨ï¼Œåˆ™å°è¯•ä¸‹è½½
        print(f"âš ï¸ spaCy æ¨¡åž‹ '{model_name}' æœªæ‰¾åˆ°ã€‚å°è¯•ä¸‹è½½...")
        try:
            # æž„å»ºä¸‹è½½å‘½ä»¤ã€‚ä½¿ç”¨ sys.executable ç¡®ä¿ä½¿ç”¨å½“å‰çŽ¯å¢ƒçš„ Python è§£é‡Šå™¨ã€‚
            command = [sys.executable, "-m", "spacy", "download", model_name]
            print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(command)}")

            # æ‰§è¡Œå‘½ä»¤
            # check=True: å¦‚æžœå‘½ä»¤è¿”å›žéžé›¶é€€å‡ºç ï¼ˆè¡¨ç¤ºå¤±è´¥ï¼‰ï¼Œåˆ™æŠ›å‡º CalledProcessError
            # capture_output=True: æ•èŽ·æ ‡å‡†è¾“å‡ºå’Œæ ‡å‡†é”™è¯¯
            # text=True: å°†è¾“å‡ºè§£ç ä¸ºæ–‡æœ¬
            result = subprocess.run(command, check=True, capture_output=True, text=True)

            print(f"ðŸŽ‰ æ¨¡åž‹ '{model_name}' ä¸‹è½½æˆåŠŸï¼")
            if result.stdout:
                print("--- ä¸‹è½½è¾“å‡º (STDOUT) ---")
                print(result.stdout)
            if result.stderr:
                print("--- ä¸‹è½½é”™è¯¯ (STDERR) ---")
                print(result.stderr)

            # ä¸‹è½½æˆåŠŸåŽå†æ¬¡å°è¯•åŠ è½½æ¨¡åž‹
            nlp = spacy.load(model_name)
            print(f"âœ… spaCy æ¨¡åž‹ '{model_name}' å·²æˆåŠŸä¸‹è½½å¹¶åŠ è½½ã€‚")
            return nlp

        except subprocess.CalledProcessError as e:
            print(f"âŒ ä¸‹è½½æ¨¡åž‹ '{model_name}' å¤±è´¥ã€‚")
            print(f"å‘½ä»¤: {e.cmd}")
            print(f"è¿”å›žç : {e.returncode}")
            print(f"STDOUT: {e.stdout}")
            print(f"STDERR: {e.stderr}")
            sys.exit(1) # ä¸‹è½½å¤±è´¥ï¼Œé€€å‡ºè„šæœ¬
        except Exception as e:
            print(f"âŒ åœ¨ä¸‹è½½æˆ–åŠ è½½æ¨¡åž‹ '{model_name}' è¿‡ç¨‹ä¸­å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
            sys.exit(1) # å‘ç”Ÿå…¶ä»–é”™è¯¯ï¼Œé€€å‡ºè„šæœ¬

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from literature_analyzer.data_processing import (
    load_and_process_data,
    get_data_hash,
    get_processing_stats,
    ProcessingError
)
from literature_analyzer.nlp_analysis import (
    generate_embeddings,
    perform_umap,
    perform_topic_modeling,
    load_embedding_model,
    AVAILABLE_MODELS,
    get_model_info,
    # æ–°å¢žä¸»é¢˜åˆ†æžæ¨¡åž‹
    AVAILABLE_TOPIC_MODELS,
    get_topic_model_info,
    perform_lda_topic_modeling,
    perform_nmf_topic_modeling,
    perform_kmeans_topic_modeling,
    perform_hdbscan_topic_modeling,
    perform_topic_modeling_ensemble,
    compare_topic_models,
    # æ–°å¢žAIåŠŸèƒ½
    perform_semantic_search,
    answer_question_with_llm,
    generate_llm_topic_label,
    extract_keywords_advanced,
    # ä¸»é¢˜æ¨¡åž‹å·¥åŽ‚
    perform_topic_modeling_with_factory,
    ModelLoadError,
    EmbeddingError,
    DimensionalityReductionError,
    TopicModelError
)
from literature_analyzer.visualization import (
    create_2d_scatter,
    create_3d_scatter,
    create_topic_distribution,
    create_wordcloud,
    create_journal_comparison,
    create_temporal_trends,
    create_summary_stats,
    # æ–°å¢žä¸»é¢˜æ¨¡åž‹å¯è§†åŒ–
    create_classical_wordcloud,
    create_topic_model_comparison,
    create_topic_similarity_heatmap,
    create_clustering_analysis,
    create_topic_evolution,
    create_model_performance_radar,
    create_topic_keyword_network,
    # æ–°å¢žAIåŠŸèƒ½å¯è§†åŒ–
    create_semantic_search_results,
    create_llm_qa_visualization,
    create_temporal_evolution_heatmap,
    create_keyword_treemap,
    create_cooccurrence_network,
    create_topic_llm_label_comparison,
    create_embedding_similarity_matrix,
    create_ai_enhanced_topic_summary,
    # æ–°å¢žå¼ºåŒ–è¯­ä¹‰ç©ºé—´å¯è§†åŒ–åŠŸèƒ½
    create_multi_dimensional_semantic_space,
    create_semantic_space_3d_interactive,
    create_semantic_similarity_network,
    create_semantic_evolution_animation,
    create_semantic_density_heatmap,
    create_semantic_clustering_analysis,
    create_semantic_drift_analysis,
    VisualizationError
)

# å¯¼å…¥æ–°çš„è¯­ä¹‰ç©ºé—´åˆ†æžæ¨¡å—
from literature_analyzer.semantic_space_analyzer import (
    SemanticSpaceAnalyzer,
    TemporalSemanticAnalyzer,
    InteractiveSemanticExplorer,
    compare_dimensionality_reduction_methods,
    create_semantic_drift_comparison,
)

# è¾…åŠ©å‡½æ•°
def build_cooccurrence_matrix(items: List[str], min_cooccurrence: int = 2) -> Dict[str, Dict[str, int]]:
    """æž„å»ºå…±çŽ°çŸ©é˜µ"""
    from collections import defaultdict, Counter
    import itertools
    
    # ç»Ÿè®¡æ¯ä¸ªé¡¹ç›®çš„å‡ºçŽ°
    item_counts = Counter(items)
    
    # æž„å»ºå…±çŽ°çŸ©é˜µ
    cooccurrence = defaultdict(lambda: defaultdict(int))
    
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®žé™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„å…±çŽ°è®¡ç®—é€»è¾‘
    # ä¾‹å¦‚ï¼šæ ¹æ®æ–‡æ¡£ä¸­çš„å…±çŽ°å…³ç³»æ¥è®¡ç®—
    unique_items = list(item_counts.keys())
    
    for i, item1 in enumerate(unique_items):
        for j, item2 in enumerate(unique_items):
            if i != j:
                # ç®€åŒ–çš„å…±çŽ°è®¡ç®—ï¼ˆå®žé™…åº”ç”¨ä¸­åº”è¯¥åŸºäºŽæ–‡æ¡£å…±çŽ°ï¼‰
                cooccurrence_count = min(item_counts[item1], item_counts[item2]) // 10
                if cooccurrence_count >= min_cooccurrence:
                    cooccurrence[item1][item2] = cooccurrence_count
    
    return dict(cooccurrence)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ–‡çŒ®åˆ†æžåº”ç”¨",
    page_icon="ðŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
def init_session_state():
    """åˆå§‹åŒ–Streamlitä¼šè¯çŠ¶æ€"""
    if 'processed_data' not in st.session_state:
        st.session_state.processed_data = None
    if 'embeddings' not in st.session_state:
        st.session_state.embeddings = None
    if 'reduced_embeddings' not in st.session_state:
        st.session_state.reduced_embeddings = None
    if 'topic_model' not in st.session_state:
        st.session_state.topic_model = None
    if 'topics_df' not in st.session_state:
        st.session_state.topics_df = None
    if 'analysis_step' not in st.session_state:
        st.session_state.analysis_step = 0
    if 'data_hash' not in st.session_state:
        st.session_state.data_hash = None
    if 'processing_stats' not in st.session_state:
        st.session_state.processing_stats = None

# ç¼“å­˜è£…é¥°å™¨
@cache_resource
def cached_load_model(model_name: str):
    """ç¼“å­˜çš„æ¨¡åž‹åŠ è½½å‡½æ•°"""
    return load_embedding_model(model_name)

@cache_data
def cached_process_data(files_hash: str, uploaded_files: List):
    """ç¼“å­˜çš„æ•°æ®å¤„ç†å‡½æ•°"""
    return load_and_process_data(uploaded_files)

@cache_data
def cached_generate_embeddings(texts_hash: str, texts: List[str], model_name: str):
    """ç¼“å­˜çš„åµŒå…¥ç”Ÿæˆå‡½æ•°"""
    return generate_embeddings(texts, model_name)

@cache_data
def cached_perform_umap(embeddings_hash: str, embeddings: np.ndarray, n_neighbors: int, min_dist: float, n_components: int):
    """ç¼“å­˜çš„UMAPé™ç»´å‡½æ•°"""
    return perform_umap(embeddings, n_neighbors, min_dist, n_components)

@cache_data
def cached_perform_topic_modeling(texts_hash: str, texts: List[str], embeddings: np.ndarray, min_topic_size: int, nr_topics: Optional[int]):
    """ç¼“å­˜çš„ä¸»é¢˜å»ºæ¨¡å‡½æ•°"""
    return perform_topic_modeling(texts, embeddings, min_topic_size, nr_topics)

def main():
    """ä¸»åº”ç”¨å‡½æ•°"""
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    init_session_state()
    
    # åº”ç”¨æ ‡é¢˜
    st.title("ðŸ“š æ–‡çŒ®åˆ†æžåº”ç”¨")
    st.markdown("---")
    
    # ä¾§è¾¹æ  - æŽ§åˆ¶é¢æ¿
    with st.sidebar:
        st.header("ðŸ“Š æ•°æ®åŠ è½½ä¸Žåˆ†æž")
        
        # ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®ä¸Šä¼ ä¸Žå¤„ç†
        uploaded_files = st.file_uploader(
            "ä¸Šä¼ WoS Excelæ–‡ä»¶",
            type=['xlsx', 'xls'],
            accept_multiple_files=True,
            help="è¯·ä¸Šä¼ ä»ŽWeb of Scienceå¯¼å‡ºçš„Full Record Excelæ–‡ä»¶"
        )
        
        if uploaded_files and st.button("1. åŠ è½½å¹¶å¤„ç†æ•°æ®", type="primary"):
            process_uploaded_data(uploaded_files)
        
        # ç¬¬äºŒéƒ¨åˆ†ï¼šåµŒå…¥ä¸Žé™ç»´
        if st.session_state.analysis_step >= 1:
            st.markdown("---")
            st.subheader("ðŸ”¤ åµŒå…¥ä¸Žé™ç»´")
            
            # æ¨¡åž‹é€‰æ‹©
            model_options = list(AVAILABLE_MODELS.keys())
            selected_model = st.selectbox(
                "é€‰æ‹©åµŒå…¥æ¨¡åž‹",
                model_options,
                format_func=lambda x: f"{x} - {get_model_info(x).get('description', '')}"
            )
            
            # æ˜¾ç¤ºæ¨¡åž‹ä¿¡æ¯
            model_info = get_model_info(selected_model)
            with st.expander("æ¨¡åž‹è¯¦ç»†ä¿¡æ¯"):
                st.write(f"**ç±»åž‹**: {model_info.get('type', 'N/A')}")
                st.write(f"**ç»´åº¦**: {model_info.get('dimensions', 'N/A')}")
                st.write(f"**æœ€å¤§é•¿åº¦**: {model_info.get('max_length', 'N/A')}")
                st.write(f"**é€Ÿåº¦**: {model_info.get('speed', 'N/A')}")
                st.write(f"**è´¨é‡**: {model_info.get('quality', 'N/A')}")
                st.write(f"**æŽ¨èç”¨é€”**: {model_info.get('recommended_for', 'N/A')}")
                
                # æ˜¾ç¤ºè®­ç»ƒè¦æ±‚æç¤º
                if model_info.get('training_required'):
                    st.warning("âš ï¸ æ­¤æ¨¡åž‹éœ€è¦è®­ç»ƒæ•°æ®ï¼Œå°†ä½¿ç”¨æ‚¨çš„æ–‡æœ¬æ•°æ®è¿›è¡Œè®­ç»ƒ")
            
            # UMAPå‚æ•°
            n_neighbors = st.slider(
                "UMAP n_neighbors",
                min_value=5,
                max_value=50,
                value=15,
                help="æŽ§åˆ¶å±€éƒ¨ç»“æž„ä¸Žå…¨å±€ç»“æž„çš„å¹³è¡¡"
            )
            
            min_dist = st.slider(
                "UMAP min_dist",
                min_value=0.0,
                max_value=0.99,
                value=0.1,
                step=0.01,
                help="æŽ§åˆ¶ç‚¹ä¹‹é—´çš„ç´§å¯†ç¨‹åº¦"
            )
            
            n_components = st.selectbox(
                "ç›®æ ‡ç»´åº¦",
                [2, 3],
                format_func=lambda x: f"{x}D"
            )
            
            if st.button("2. æ‰§è¡ŒåµŒå…¥ä¸Žé™ç»´", type="primary"):
                perform_embedding_and_umap(selected_model, n_neighbors, min_dist, n_components)
        
        # ç¬¬ä¸‰éƒ¨åˆ†ï¼šä¸»é¢˜å»ºæ¨¡
        if st.session_state.analysis_step >= 2:
            st.markdown("---")
            st.subheader("ðŸ·ï¸ ä¸»é¢˜å»ºæ¨¡")
            
            # ä¸»é¢˜æ¨¡åž‹é€‰æ‹©
            topic_model_options = list(AVAILABLE_TOPIC_MODELS.keys())
            selected_topic_model = st.selectbox(
                "é€‰æ‹©ä¸»é¢˜æ¨¡åž‹",
                topic_model_options,
                format_func=lambda x: f"{x} - {get_topic_model_info(x).get('description', '')}"
            )
            
            # æ˜¾ç¤ºä¸»é¢˜æ¨¡åž‹ä¿¡æ¯
            topic_model_info = get_topic_model_info(selected_topic_model)
            with st.expander("æ¨¡åž‹è¯¦ç»†ä¿¡æ¯"):
                st.write(f"**ç±»åž‹**: {topic_model_info.get('type', 'N/A')}")
                st.write(f"**ç®—æ³•**: {topic_model_info.get('algorithm', 'N/A')}")
                st.write(f"**ä¼˜ç‚¹**: {topic_model_info.get('advantages', 'N/A')}")
                st.write(f"**ç¼ºç‚¹**: {topic_model_info.get('disadvantages', 'N/A')}")
                st.write(f"**é€‚ç”¨åœºæ™¯**: {topic_model_info.get('suitable_for', 'N/A')}")
            
            # æ ¹æ®ä¸åŒæ¨¡åž‹æ˜¾ç¤ºä¸åŒå‚æ•°
            if selected_topic_model == "BERTopic":
                min_topic_size = st.slider(
                    "æœ€å°ä¸»é¢˜å¤§å°",
                    min_value=5,
                    max_value=100,
                    value=10,
                    help="æ¯ä¸ªä¸»é¢˜è‡³å°‘åŒ…å«çš„æ–‡æ¡£æ•°é‡"
                )
                
                nr_topics_option = st.selectbox(
                    "ä¸»é¢˜æ•°é‡",
                    ["auto", "10", "15", "20", "25", "30"],
                    format_func=lambda x: f"{x} ä¸ªä¸»é¢˜" if x != "auto" else "è‡ªåŠ¨ç¡®å®š"
                )
                
                nr_topics = None if nr_topics_option == "auto" else int(nr_topics_option)
                
                if st.button("3. æ‰§è¡Œä¸»é¢˜åˆ†æž", type="primary"):
                    perform_topic_analysis(selected_topic_model, min_topic_size=min_topic_size, nr_topics=nr_topics)
                    
            elif selected_topic_model == "LDA":
                n_topics = st.slider(
                    "ä¸»é¢˜æ•°é‡",
                    min_value=2,
                    max_value=50,
                    value=10,
                    help="LDAæ¨¡åž‹çš„ä¸»é¢˜æ•°é‡"
                )
                
                random_state = st.number_input(
                    "éšæœºç§å­",
                    min_value=0,
                    max_value=1000,
                    value=42,
                    help="ç¡®ä¿ç»“æžœå¯é‡çŽ°"
                )
                
                if st.button("3. æ‰§è¡Œä¸»é¢˜åˆ†æž", type="primary"):
                    perform_topic_analysis(selected_topic_model, n_topics=n_topics, random_state=random_state)
                    
            elif selected_topic_model == "NMF":
                n_topics = st.slider(
                    "ä¸»é¢˜æ•°é‡",
                    min_value=2,
                    max_value=50,
                    value=10,
                    help="NMFæ¨¡åž‹çš„ä¸»é¢˜æ•°é‡"
                )
                
                random_state = st.number_input(
                    "éšæœºç§å­",
                    min_value=0,
                    max_value=1000,
                    value=42,
                    help="ç¡®ä¿ç»“æžœå¯é‡çŽ°"
                )
                
                if st.button("3. æ‰§è¡Œä¸»é¢˜åˆ†æž", type="primary"):
                    perform_topic_analysis(selected_topic_model, n_topics=n_topics, random_state=random_state)
                    
            elif selected_topic_model == "KMeans":
                n_clusters = st.slider(
                    "èšç±»æ•°é‡",
                    min_value=2,
                    max_value=50,
                    value=10,
                    help="KMeansçš„èšç±»æ•°é‡"
                )
                
                random_state = st.number_input(
                    "éšæœºç§å­",
                    min_value=0,
                    max_value=1000,
                    value=42,
                    help="ç¡®ä¿ç»“æžœå¯é‡çŽ°"
                )
                
                if st.button("3. æ‰§è¡Œä¸»é¢˜åˆ†æž", type="primary"):
                    perform_topic_analysis(selected_topic_model, n_clusters=n_clusters, random_state=random_state)
                    
            elif selected_topic_model == "HDBSCAN":
                min_cluster_size = st.slider(
                    "æœ€å°èšç±»å¤§å°",
                    min_value=2,
                    max_value=50,
                    value=5,
                    help="HDBSCANçš„æœ€å°èšç±»å¤§å°"
                )
                
                min_samples = st.slider(
                    "æœ€å°æ ·æœ¬æ•°",
                    min_value=1,
                    max_value=20,
                    value=1,
                    help="HDBSCANçš„æœ€å°æ ·æœ¬æ•°"
                )
                
                if st.button("3. æ‰§è¡Œä¸»é¢˜åˆ†æž", type="primary"):
                    perform_topic_analysis(selected_topic_model, min_cluster_size=min_cluster_size, min_samples=min_samples)
                    
            elif selected_topic_model == "Ensemble":
                # é›†æˆæ¨¡åž‹å‚æ•°
                min_topic_size = st.slider(
                    "æœ€å°ä¸»é¢˜å¤§å°",
                    min_value=5,
                    max_value=100,
                    value=10,
                    help="æ¯ä¸ªä¸»é¢˜è‡³å°‘åŒ…å«çš„æ–‡æ¡£æ•°é‡"
                )
                
                nr_topics_option = st.selectbox(
                    "ä¸»é¢˜æ•°é‡",
                    ["auto", "10", "15", "20", "25", "30"],
                    format_func=lambda x: f"{x} ä¸ªä¸»é¢˜" if x != "auto" else "è‡ªåŠ¨ç¡®å®š"
                )
                
                nr_topics = None if nr_topics_option == "auto" else int(nr_topics_option)
                
                # é€‰æ‹©è¦é›†æˆçš„æ¨¡åž‹
                ensemble_models = st.multiselect(
                    "é€‰æ‹©è¦é›†æˆçš„æ¨¡åž‹",
                    ["BERTopic", "LDA", "NMF", "KMeans"],
                    default=["BERTopic", "LDA"]
                )
                
                if st.button("3. æ‰§è¡Œä¸»é¢˜åˆ†æž", type="primary"):
                    perform_topic_analysis(
                        selected_topic_model, 
                        min_topic_size=min_topic_size, 
                        nr_topics=nr_topics,
                        ensemble_models=ensemble_models
                    )
        
        # ç¬¬å››éƒ¨åˆ†ï¼šå›¾è¡¨ç­›é€‰
        if st.session_state.analysis_step >= 3:
            st.markdown("---")
            st.header("ðŸ” å›¾è¡¨ç­›é€‰")
            
            # å¹´ä»½ç­›é€‰
            if st.session_state.processed_data is not None:
                year_range = st.session_state.processed_data['publication_year'].agg(['min', 'max'])
                selected_years = st.slider(
                    "ç­›é€‰å¹´ä»½èŒƒå›´",
                    int(year_range['min']),
                    int(year_range['max']),
                    (int(year_range['min']), int(year_range['max']))
                )
            
            # æœŸåˆŠç­›é€‰
            available_journals = sorted(st.session_state.processed_data['journal_title'].unique())
            selected_journals = st.multiselect(
                "ç­›é€‰æœŸåˆŠ",
                available_journals,
                default=available_journals[:10] if len(available_journals) > 10 else available_journals
            )
            
            # ä¸»é¢˜ç­›é€‰
            if st.session_state.topics_df is not None:
                available_topics = sorted(st.session_state.topics_df['topic_name'].unique())
                selected_topics = st.multiselect(
                    "ç­›é€‰ä¸»é¢˜",
                    available_topics,
                    default=[t for t in available_topics if t != "Unclassified"]
                )
            
            # å…³é”®è¯æœç´¢
            search_keywords = st.text_input("å…³é”®è¯æœç´¢")
        
        # ç¬¬äº”éƒ¨åˆ†ï¼šå·¥å…·
        st.markdown("---")
        st.header("ðŸ› ï¸ å·¥å…·")
        
        if st.button("æ¸…é™¤æ‰€æœ‰ç¼“å­˜å¹¶é‡ç½®"):
            clear_all_cache()
            st.rerun()
    
    # ä¸»å†…å®¹åŒºåŸŸ
    if st.session_state.analysis_step == 0:
        show_welcome_page()
    else:
        # åˆ›å»ºæ ‡ç­¾é¡µ
        tabs = st.tabs([
            "ðŸ“‹ æ•°æ®æ¦‚è§ˆ",
            "ðŸŒ è¯­ä¹‰ç©ºé—´æŽ¢ç´¢", 
            "ðŸ·ï¸ ä¸»é¢˜åˆ†æž",
            "ðŸ“° æœŸåˆŠå¯¹æ¯”",
            "ðŸ“ˆ æ—¶é—´è¶‹åŠ¿",
            "ðŸ¤– AIåŠŸèƒ½",
            "ðŸ“Š é«˜çº§åˆ†æž"
        ])
        
        # èŽ·å–ç­›é€‰åŽçš„æ•°æ®
        filtered_data = get_filtered_data()
        
        with tabs[0]:
            show_data_overview(filtered_data)
        
        with tabs[1]:
            show_semantic_space(filtered_data)
        
        with tabs[2]:
            show_topic_analysis(filtered_data)
        
        with tabs[3]:
            show_journal_comparison(filtered_data)
        
        with tabs[4]:
            show_temporal_trends(filtered_data)
        
        with tabs[5]:
            show_ai_functions(filtered_data)
        
        with tabs[6]:
            show_advanced_analysis(filtered_data)

def process_uploaded_data(uploaded_files: List):
    """å¤„ç†ä¸Šä¼ çš„æ•°æ®æ–‡ä»¶"""
    try:
        with st.spinner("æ­£åœ¨å¤„ç†æ•°æ®..."):
            # è®¡ç®—æ•°æ®å“ˆå¸Œ
            data_hash = get_data_hash(uploaded_files)
            
            # æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜
            if st.session_state.data_hash == data_hash and st.session_state.processed_data is not None:
                st.info("ä½¿ç”¨ç¼“å­˜çš„å¤„ç†ç»“æžœ")
            else:
                # å¤„ç†æ•°æ®
                processed_data = cached_process_data(data_hash, uploaded_files)
                
                # æ›´æ–°ä¼šè¯çŠ¶æ€
                st.session_state.processed_data = processed_data
                st.session_state.data_hash = data_hash
                st.session_state.analysis_step = 1
                
                # è®¡ç®—å¤„ç†ç»Ÿè®¡
                original_count = sum(len(pd.read_excel(f)) for f in uploaded_files)
                st.session_state.processing_stats = get_processing_stats(
                    pd.DataFrame({'count': [original_count]}),
                    processed_data
                )
            
            st.success("æ•°æ®å¤„ç†å®Œæˆï¼")
            st.rerun()
            
    except ProcessingError as e:
        st.error(f"æ•°æ®å¤„ç†å¤±è´¥: {str(e)}")
    except Exception as e:
        st.error(f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
        logger.error(f"æ•°æ®å¤„ç†é”™è¯¯: {e}")

def perform_embedding_and_umap(model_name: str, n_neighbors: int, min_dist: float, n_components: int):
    """æ‰§è¡ŒåµŒå…¥å’Œé™ç»´"""
    try:
        with st.spinner("æ­£åœ¨ç”ŸæˆåµŒå…¥å‘é‡..."):
            texts = st.session_state.processed_data['processed_text'].tolist()
            texts_hash = hash(str(texts))
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            embeddings = cached_generate_embeddings(texts_hash, texts, model_name)
            st.session_state.embeddings = embeddings
            
        with st.spinner("æ­£åœ¨æ‰§è¡ŒUMAPé™ç»´..."):
            # æ‰§è¡ŒUMAPé™ç»´
            embeddings_hash = hash(embeddings.tobytes())
            reduced_embeddings = cached_perform_umap(
                embeddings_hash, embeddings, n_neighbors, min_dist, n_components
            )
            st.session_state.reduced_embeddings = reduced_embeddings
            
            # æ·»åŠ åæ ‡åˆ°æ•°æ®ä¸­ - å…ˆåˆ é™¤å·²å­˜åœ¨çš„åæ ‡åˆ—ä»¥é¿å…é‡å¤
            coord_columns = ['x', 'y', 'z']
            existing_coord_columns = [col for col in coord_columns if col in st.session_state.processed_data.columns]
            if existing_coord_columns:
                st.session_state.processed_data = st.session_state.processed_data.drop(columns=existing_coord_columns)
            
            if n_components == 3:
                st.session_state.processed_data['x'] = reduced_embeddings[:, 0]
                st.session_state.processed_data['y'] = reduced_embeddings[:, 1]
                st.session_state.processed_data['z'] = reduced_embeddings[:, 2]
            else:
                st.session_state.processed_data['x'] = reduced_embeddings[:, 0]
                st.session_state.processed_data['y'] = reduced_embeddings[:, 1]
                st.session_state.processed_data['z'] = 0
            
            st.session_state.analysis_step = 2
            st.success("åµŒå…¥ä¸Žé™ç»´å®Œæˆï¼")
            st.rerun()
            
    except (ModelLoadError, EmbeddingError, DimensionalityReductionError) as e:
        st.error(f"åˆ†æžå¤±è´¥: {str(e)}")
    except Exception as e:
        st.error(f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
        logger.error(f"åµŒå…¥ä¸Žé™ç»´é”™è¯¯: {e}")

def perform_topic_analysis(model_type: str, **kwargs):
    """æ‰§è¡Œä¸»é¢˜åˆ†æž"""
    try:
        with st.spinner(f"æ­£åœ¨æ‰§è¡Œ{model_type}ä¸»é¢˜å»ºæ¨¡..."):
            texts = st.session_state.processed_data['processed_text'].tolist()
            embeddings = st.session_state.embeddings
            texts_hash = hash(str(texts))
            
            # æ ¹æ®æ¨¡åž‹ç±»åž‹æ‰§è¡Œä¸åŒçš„ä¸»é¢˜å»ºæ¨¡
            if model_type == "BERTopic":
                min_topic_size = kwargs.get('min_topic_size', 10)
                nr_topics = kwargs.get('nr_topics', None)
                topic_model, topics_df = cached_perform_topic_modeling(
                    texts_hash, texts, embeddings, min_topic_size, nr_topics
                )
                
            elif model_type == "LDA":
                n_topics = kwargs.get('n_topics', 10)
                random_state = kwargs.get('random_state', 42)
                topic_model, topics_df = perform_lda_topic_modeling(
                    texts, n_topics=n_topics, random_state=random_state
                )
                
            elif model_type == "NMF":
                n_topics = kwargs.get('n_topics', 10)
                random_state = kwargs.get('random_state', 42)
                topic_model, topics_df = perform_nmf_topic_modeling(
                    texts, n_topics=n_topics, random_state=random_state
                )
                
            elif model_type == "KMeans":
                n_clusters = kwargs.get('n_clusters', 10)
                random_state = kwargs.get('random_state', 42)
                topic_model, topics_df = perform_kmeans_topic_modeling(
                    texts, embeddings, n_clusters=n_clusters, random_state=random_state
                )
                
            elif model_type == "HDBSCAN":
                min_cluster_size = kwargs.get('min_cluster_size', 5)
                min_samples = kwargs.get('min_samples', 1)
                topic_model, topics_df = perform_hdbscan_topic_modeling(
                    texts, embeddings, min_cluster_size=min_samples, min_samples=min_samples
                )
                
            elif model_type == "Ensemble":
                min_topic_size = kwargs.get('min_topic_size', 10)
                nr_topics = kwargs.get('nr_topics', None)
                ensemble_models = kwargs.get('ensemble_models', ['BERTopic', 'LDA'])
                topic_model, topics_df = perform_topic_modeling_ensemble(
                    texts, embeddings, min_topic_size=min_topic_size, nr_topics=nr_topics,
                    ensemble_models=ensemble_models
                )
                
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ä¸»é¢˜æ¨¡åž‹ç±»åž‹: {model_type}")
            
            st.session_state.topic_model = topic_model
            st.session_state.topics_df = topics_df
            st.session_state.current_topic_model = model_type
            
            # åˆå¹¶ä¸»é¢˜ä¿¡æ¯åˆ°ä¸»æ•°æ® - å…ˆåˆ é™¤å·²å­˜åœ¨çš„ä¸»é¢˜åˆ—ä»¥é¿å…é‡å¤
            st.session_state.processed_data = st.session_state.processed_data.reset_index(drop=True)
            
            # åˆ é™¤å·²å­˜åœ¨çš„ä¸»é¢˜åˆ—ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
            columns_to_drop = ['topic_id', 'topic_name', 'topic_probability']
            existing_columns = [col for col in columns_to_drop if col in st.session_state.processed_data.columns]
            if existing_columns:
                st.session_state.processed_data = st.session_state.processed_data.drop(columns=existing_columns)
            
            # æ·»åŠ æ–°çš„ä¸»é¢˜åˆ—
            st.session_state.processed_data = pd.concat([
                st.session_state.processed_data,
                topics_df[['topic_id', 'topic_name', 'topic_probability']]
            ], axis=1)
            
            st.session_state.analysis_step = 3
            st.success(f"{model_type}ä¸»é¢˜åˆ†æžå®Œæˆï¼")
            st.rerun()
            
    except TopicModelError as e:
        st.error(f"ä¸»é¢˜å»ºæ¨¡å¤±è´¥: {str(e)}")
    except Exception as e:
        st.error(f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
        logger.error(f"ä¸»é¢˜åˆ†æžé”™è¯¯: {e}")

def get_filtered_data() -> pd.DataFrame:
    """èŽ·å–åº”ç”¨ç­›é€‰æ¡ä»¶åŽçš„æ•°æ®"""
    if st.session_state.processed_data is None:
        return pd.DataFrame()
    
    filtered_data = st.session_state.processed_data.copy()
    
    # å¹´ä»½ç­›é€‰
    if 'selected_years' in st.session_state:
        year_min, year_max = st.session_state.selected_years
        filtered_data = filtered_data[
            (filtered_data['publication_year'] >= year_min) &
            (filtered_data['publication_year'] <= year_max)
        ]
    
    # æœŸåˆŠç­›é€‰
    if 'selected_journals' in st.session_state and st.session_state.selected_journals:
        filtered_data = filtered_data[
            filtered_data['journal_title'].isin(st.session_state.selected_journals)
        ]
    
    # ä¸»é¢˜ç­›é€‰
    if 'selected_topics' in st.session_state and st.session_state.selected_topics:
        filtered_data = filtered_data[
            filtered_data['topic_name'].isin(st.session_state.selected_topics)
        ]
    
    # å…³é”®è¯æœç´¢
    if 'search_keywords' in st.session_state and st.session_state.search_keywords:
        keywords = st.session_state.search_keywords.lower()
        mask = (
            filtered_data['article_title'].str.lower().str.contains(keywords, na=False) |
            filtered_data['abstract_text'].str.lower().str.contains(keywords, na=False)
        )
        filtered_data = filtered_data[mask]
    
    return filtered_data

def show_welcome_page():
    """æ˜¾ç¤ºæ¬¢è¿Žé¡µé¢"""
    st.markdown("""
    ## ðŸŽ¯ æ¬¢è¿Žä½¿ç”¨æ–‡çŒ®åˆ†æžåº”ç”¨
    
    è¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºç§‘ç ”äººå‘˜è®¾è®¡çš„äº¤äº’å¼æ–‡çŒ®åˆ†æžå·¥å…·ï¼Œèƒ½å¤Ÿå¸®åŠ©æ‚¨ï¼š
    
    - ðŸ” **æ·±åº¦åˆ†æž**ï¼šä»ŽWeb of Scienceå¯¼å‡ºçš„æ–‡çŒ®æ•°æ®ä¸­å‘çŽ°éšè—çš„æ¨¡å¼å’Œè¶‹åŠ¿
    - ðŸ“Š **å¯è§†åŒ–æŽ¢ç´¢**ï¼šé€šè¿‡ä¸°å¯Œçš„äº¤äº’å¼å›¾è¡¨ç›´è§‚ç†è§£æ–‡çŒ®ç»“æž„
    - ðŸ·ï¸ **ä¸»é¢˜å‘çŽ°**ï¼šè‡ªåŠ¨è¯†åˆ«ç ”ç©¶ä¸»é¢˜å’Œçƒ­ç‚¹é¢†åŸŸ
    - ðŸ“° **æœŸåˆŠå¯¹æ¯”**ï¼šåˆ†æžä¸åŒæœŸåˆŠçš„å­¦æœ¯ç‰¹è‰²å’Œå†…å®¹å·®å¼‚
    
    ### ðŸš€ å¿«é€Ÿå¼€å§‹
    
    1. **ä¸Šä¼ æ•°æ®**ï¼šåœ¨å·¦ä¾§è¾¹æ ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ªWoS Excelæ–‡ä»¶
    2. **æ•°æ®å¤„ç†**ï¼šç‚¹å‡»"åŠ è½½å¹¶å¤„ç†æ•°æ®"æŒ‰é’®
    3. **åµŒå…¥é™ç»´**ï¼šé€‰æ‹©æ¨¡åž‹å’Œå‚æ•°ï¼Œæ‰§è¡ŒåµŒå…¥ä¸Žé™ç»´
    4. **ä¸»é¢˜åˆ†æž**ï¼šé…ç½®ä¸»é¢˜å‚æ•°ï¼Œæ‰§è¡Œä¸»é¢˜å»ºæ¨¡
    5. **æŽ¢ç´¢ç»“æžœ**ï¼šåœ¨ä¸åŒæ ‡ç­¾é¡µä¸­æŸ¥çœ‹åˆ†æžç»“æžœ
    
    ### ðŸ“‹ æ•°æ®è¦æ±‚
    
    ç¡®ä¿æ‚¨çš„Excelæ–‡ä»¶åŒ…å«ä»¥ä¸‹å¿…éœ€åˆ—ï¼š
    - `Article Title`ï¼ˆæ–‡ç« æ ‡é¢˜ï¼‰
    - `Source Title`ï¼ˆæœŸåˆŠæ ‡é¢˜ï¼‰
    - `Publication Year`ï¼ˆå‘è¡¨å¹´ä»½ï¼‰
    - `Abstract`ï¼ˆæ‘˜è¦ï¼‰
    
    ### ðŸ’¡ ä½¿ç”¨æç¤º
    
    - æ”¯æŒåŒæ—¶ä¸Šä¼ å¤šä¸ªExcelæ–‡ä»¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨åˆå¹¶
    - æ‰€æœ‰åˆ†æžæ­¥éª¤éƒ½æ”¯æŒå‚æ•°è°ƒæ•´å’Œå®žæ—¶é¢„è§ˆ
    - æä¾›ä¸°å¯Œçš„ç­›é€‰å’Œæœç´¢åŠŸèƒ½
    - å¯ä»¥å¯¼å‡ºå®Œæ•´çš„åˆ†æžç»“æžœ
    
    ---
    
    *å¼€å§‹æ‚¨çš„æ–‡çŒ®åˆ†æžä¹‹æ—…å§ï¼*
    """)

def show_data_overview(data: pd.DataFrame):
    """æ˜¾ç¤ºæ•°æ®æ¦‚è§ˆ"""
    st.subheader("ðŸ“‹ æ•°æ®æ¦‚è§ˆ")
    
    if data.empty:
        st.warning("æ²¡æœ‰å¯æ˜¾ç¤ºçš„æ•°æ®")
        return
    
    # å¤„ç†ç»Ÿè®¡
    if st.session_state.processing_stats:
        stats = st.session_state.processing_stats
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("åŽŸå§‹è®°å½•æ•°", stats['original_count'])
        with col2:
            st.metric("å¤„ç†åŽè®°å½•æ•°", stats['processed_count'])
        with col3:
            retention_rate = stats['retention_rate'] * 100
            st.metric("ä¿ç•™çŽ‡", f"{retention_rate:.1f}%")
        with col4:
            st.metric("æœŸåˆŠæ•°é‡", stats['journal_count'])
        
        st.markdown("---")
    
    # æ•°æ®è¡¨æ ¼
    st.subheader("ðŸ“„ æ•°æ®é¢„è§ˆ")
    
    # é€‰æ‹©æ˜¾ç¤ºçš„åˆ—
    display_columns = [
        'article_title', 'journal_title', 'publication_year', 
        'topic_name', 'topic_probability'
    ]
    available_columns = [col for col in display_columns if col in data.columns]
    
    if available_columns:
        # æ˜¾ç¤ºå‰10è¡Œ
        display_data = data[available_columns].head(10)
        st.dataframe(display_data, use_container_width=True)
        
        # æ•°æ®ä¸‹è½½
        csv = data.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="ðŸ“¥ ä¸‹è½½å®Œæ•´æ•°æ® (CSV)",
            data=csv,
            file_name='literature_analysis_results.csv',
            mime='text/csv'
        )
    else:
        st.warning("æ²¡æœ‰å¯æ˜¾ç¤ºçš„æ•°æ®åˆ—")

def show_semantic_space(data: pd.DataFrame):
    """æ˜¾ç¤ºè¯­ä¹‰ç©ºé—´æŽ¢ç´¢"""
    st.subheader("ðŸŒ è¯­ä¹‰ç©ºé—´æŽ¢ç´¢")
    
    if data.empty or 'x' not in data.columns:
        st.warning("è¯·å…ˆå®ŒæˆåµŒå…¥ä¸Žé™ç»´åˆ†æž")
        return
    
    # æ£€æŸ¥æ˜¯2Dè¿˜æ˜¯3Dæ•°æ®
    is_3d = 'z' in data.columns and data['z'].nunique() > 1
    
    # é¢œè‰²æ˜ å°„é€‰æ‹© - æ ¹æ®å¯ç”¨çš„åˆ—åŠ¨æ€è°ƒæ•´é€‰é¡¹
    color_options = []
    if 'topic_name' in data.columns:
        color_options.append('topic_name')
    if 'journal_title' in data.columns:
        color_options.append('journal_title')
    if 'publication_year' in data.columns:
        color_options.append('publication_year')
    
    # å¦‚æžœæ²¡æœ‰ä¸»é¢˜ä¿¡æ¯ï¼Œæ·»åŠ æç¤º
    if 'topic_name' not in data.columns:
        st.info("ðŸ’¡ æç¤ºï¼šå®Œæˆä¸»é¢˜åˆ†æžåŽå¯ä»¥æŒ‰ä¸»é¢˜è¿›è¡Œé¢œè‰²æ˜ å°„")
    
    if color_options:
        color_mapping = st.selectbox(
            "é€‰æ‹©é¢œè‰²æ˜ å°„",
            color_options,
            key="color_mapping_semantic"
        )
        
        # èŽ·å–é€‰å®šçš„æœŸåˆŠï¼ˆç”¨äºŽå‡¸åŒ…åˆ†æžï¼‰
        selected_journals = st.session_state.get('selected_journals', [])
        
        # åˆ›å»ºæ•£ç‚¹å›¾
        try:
            if is_3d:
                fig = create_3d_scatter(
                    data,
                    color_col=color_mapping,
                    selected_journals=selected_journals
                )
            else:
                # åˆ›å»º2Dæ•£ç‚¹å›¾
                fig = create_2d_scatter(
                    data,
                    color_col=color_mapping,
                    selected_journals=selected_journals
                )
            st.plotly_chart(fig, use_container_width=True)
            
        except VisualizationError as e:
            st.error(f"å›¾è¡¨ç”Ÿæˆå¤±è´¥: {str(e)}")
    else:
        st.warning("æ²¡æœ‰å¯ç”¨çš„é¢œè‰²æ˜ å°„é€‰é¡¹ï¼Œè¯·ç¡®ä¿æ•°æ®ä¸­åŒ…å«æœŸåˆŠæˆ–å¹´ä»½ä¿¡æ¯")

def show_topic_analysis(data: pd.DataFrame):
    """æ˜¾ç¤ºä¸»é¢˜åˆ†æž"""
    st.subheader("ðŸ·ï¸ ä¸»é¢˜åˆ†æž")
    
    if data.empty or 'topic_name' not in data.columns:
        st.warning("è¯·å…ˆå®Œæˆä¸»é¢˜åˆ†æž")
        return
    
    # ä¸»é¢˜åˆ†å¸ƒå›¾
    st.subheader("ðŸ“Š ä¸»é¢˜åˆ†å¸ƒ")
    try:
        topic_fig = create_topic_distribution(data)
        st.plotly_chart(topic_fig, use_container_width=True)
    except VisualizationError as e:
        st.error(f"ä¸»é¢˜åˆ†å¸ƒå›¾ç”Ÿæˆå¤±è´¥: {str(e)}")
    
    # ä¸»é¢˜è¯äº‘
    st.subheader("â˜ï¸ ä¸»é¢˜è¯äº‘")
    
    if st.session_state.topic_model is not None and st.session_state.topics_df is not None:
        # èŽ·å–å¯ç”¨ä¸»é¢˜ï¼ˆæŽ’é™¤Unclassifiedï¼‰
        available_topics = data['topic_name'].unique()
        available_topics = [t for t in available_topics if t != "Unclassified"]
        
        if available_topics:
            selected_topic = st.selectbox("é€‰æ‹©ä¸»é¢˜", available_topics)
            
            # èŽ·å–ä¸»é¢˜ID - æ ¹æ®ä¸åŒçš„æ¨¡åž‹ç±»åž‹ä½¿ç”¨ä¸åŒçš„æ–¹æ³•
            current_model = st.session_state.current_topic_model
            
            if current_model == "BERTopic":
                # BERTopicæ¨¡åž‹æœ‰get_topic_infoæ–¹æ³•
                topic_info = st.session_state.topic_model.get_topic_info()
                topic_row = topic_info[topic_info['Name'].str.contains(selected_topic.split()[0])]
                if not topic_row.empty:
                    topic_id = topic_row.iloc[0]['Topic']
                else:
                    topic_id = -1
            else:
                # å…¶ä»–æ¨¡åž‹ï¼šä»Žtopics_dfä¸­èŽ·å–ä¸»é¢˜ID
                topic_row = st.session_state.topics_df[
                    st.session_state.topics_df['topic_name'] == selected_topic
                ]
                if not topic_row.empty:
                    topic_id = topic_row.iloc[0]['topic_id']
                else:
                    topic_id = -1
            
            if topic_id != -1:
                # ç”Ÿæˆè¯äº‘ - æ ¹æ®æ¨¡åž‹ç±»åž‹ä½¿ç”¨ä¸åŒçš„æ–¹æ³•
                try:
                    if current_model == "BERTopic":
                        wordcloud_b64 = create_wordcloud(st.session_state.topic_model, topic_id)
                    else:
                        # å¯¹äºŽç»å…¸ä¸»é¢˜æ¨¡åž‹ï¼Œä½¿ç”¨topics_dfä¸­çš„å…³é”®è¯ä¿¡æ¯
                        wordcloud_b64 = create_classical_wordcloud(
                            st.session_state.topics_df, topic_id, current_model
                        )
                    
                    if wordcloud_b64:
                        st.image(wordcloud_b64, use_container_width=True)
                    else:
                        st.warning("æ— æ³•ç”Ÿæˆè¯äº‘")
                except Exception as e:
                    st.warning(f"ç”Ÿæˆè¯äº‘æ—¶å‡ºé”™: {e}")
            else:
                st.warning("æ— æ³•æ‰¾åˆ°é€‰å®šçš„ä¸»é¢˜")
        else:
            st.warning("æ²¡æœ‰å¯ç”¨çš„ä¸»é¢˜")
    else:
        st.warning("ä¸»é¢˜æ¨¡åž‹æœªåŠ è½½æˆ–ä¸»é¢˜æ•°æ®ä¸å¯ç”¨")

def show_journal_comparison(data: pd.DataFrame):
    """æ˜¾ç¤ºæœŸåˆŠå¯¹æ¯”"""
    st.subheader("ðŸ“° æœŸåˆŠå¯¹æ¯”")
    
    if data.empty or 'topic_name' not in data.columns:
        st.warning("è¯·å…ˆå®Œæˆä¸»é¢˜åˆ†æž")
        return
    
    try:
        # æœŸåˆŠ-ä¸»é¢˜åˆ†å¸ƒå›¾
        journal_fig = create_journal_comparison(data)
        st.plotly_chart(journal_fig, use_container_width=True)
        
    except VisualizationError as e:
        st.error(f"æœŸåˆŠå¯¹æ¯”å›¾ç”Ÿæˆå¤±è´¥: {str(e)}")

def show_temporal_trends(data: pd.DataFrame):
    """æ˜¾ç¤ºæ—¶é—´è¶‹åŠ¿"""
    st.subheader("ðŸ“ˆ æ—¶é—´è¶‹åŠ¿")
    
    if data.empty or 'topic_name' not in data.columns:
        st.warning("è¯·å…ˆå®Œæˆä¸»é¢˜åˆ†æž")
        return
    
    try:
        # æ—¶é—´è¶‹åŠ¿å›¾
        temporal_fig = create_temporal_trends(data)
        st.plotly_chart(temporal_fig, use_container_width=True)
        
    except VisualizationError as e:
        st.error(f"æ—¶é—´è¶‹åŠ¿å›¾ç”Ÿæˆå¤±è´¥: {str(e)}")

def clear_all_cache():
    """æ¸…é™¤æ‰€æœ‰ç¼“å­˜"""
    # æ¸…é™¤Streamlitç¼“å­˜
    cache_data.clear()
    cache_resource.clear()
    
    # æ¸…é™¤ä¼šè¯çŠ¶æ€
    for key in list(st.session_state.keys()):
        del st.session_state[key]
    
    # é‡æ–°åˆå§‹åŒ–
    init_session_state()
    
    st.success("æ‰€æœ‰ç¼“å­˜å·²æ¸…é™¤ï¼Œåº”ç”¨å·²é‡ç½®")

def show_ai_functions(data: pd.DataFrame):
    """æ˜¾ç¤ºAIåŠŸèƒ½é¡µé¢"""
    st.subheader("ðŸ¤– AIæ™ºèƒ½åˆ†æžåŠŸèƒ½")
    
    if data.empty:
        st.warning("æ²¡æœ‰å¯åˆ†æžçš„æ•°æ®")
        return
    
    # åˆ›å»ºAIåŠŸèƒ½å­æ ‡ç­¾é¡µ
    ai_tabs = st.tabs([
        "ðŸ” è¯­ä¹‰æœç´¢",
        "ðŸ’¬ AIé—®ç­”",
        "ðŸ·ï¸ ä¸»é¢˜æ ‡ç­¾ä¼˜åŒ–"
    ])
    
    with ai_tabs[0]:
        show_semantic_search(data)
    
    with ai_tabs[1]:
        show_ai_qa(data)
    
    with ai_tabs[2]:
        show_topic_label_optimization(data)

def show_semantic_search(data: pd.DataFrame):
    """æ˜¾ç¤ºè¯­ä¹‰æœç´¢åŠŸèƒ½"""
    st.subheader("ðŸ” è¯­ä¹‰æœç´¢")
    
    if 'embeddings' not in st.session_state or st.session_state.embeddings is None:
        st.warning("è¯·å…ˆå®ŒæˆåµŒå…¥åˆ†æž")
        return
    
    # æœç´¢è¾“å…¥
    search_query = st.text_input(
        "è¾“å…¥æœç´¢æŸ¥è¯¢",
        placeholder="è¾“å…¥æ‚¨æƒ³è¦æœç´¢çš„å…³é”®è¯æˆ–é—®é¢˜..."
    )
    
    if search_query and st.button("æ‰§è¡Œè¯­ä¹‰æœç´¢", type="primary"):
        try:
            with st.spinner("æ­£åœ¨æ‰§è¡Œè¯­ä¹‰æœç´¢..."):
                # æ‰§è¡Œè¯­ä¹‰æœç´¢
                results_df = perform_semantic_search(
                    search_query, 
                    st.session_state.embeddings,
                    data
                )
                
                if not results_df.empty:
                    # æ˜¾ç¤ºæœç´¢ç»“æžœ
                    st.success(f"æ‰¾åˆ° {len(results_df)} æ¡ç›¸å…³æ–‡çŒ®")
                    
                    # åˆ›å»ºæœç´¢ç»“æžœå¯è§†åŒ–
                    fig = create_semantic_search_results(results_df, search_query)
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # æ˜¾ç¤ºè¯¦ç»†ç»“æžœ
                    st.subheader("ðŸ“„ æœç´¢ç»“æžœè¯¦æƒ…")
                    for idx, row in results_df.head(5).iterrows():
                        with st.expander(f"{row['article_title']} (ç›¸ä¼¼åº¦: {row['similarity']:.3f})"):
                            st.write(f"**æœŸåˆŠ**: {row['journal_title']}")
                            st.write(f"**å¹´ä»½**: {row['publication_year']}")
                            st.write(f"**æ‘˜è¦**: {row['abstract_text'][:300]}...")
                else:
                    st.warning("æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®")
                    
        except Exception as e:
            st.error(f"è¯­ä¹‰æœç´¢å¤±è´¥: {e}")

def show_ai_qa(data: pd.DataFrame):
    """æ˜¾ç¤ºAIé—®ç­”åŠŸèƒ½"""
    st.subheader("ðŸ’¬ AIæ™ºèƒ½é—®ç­”")
    
    # é—®é¢˜è¾“å…¥
    question = st.text_area(
        "è¾“å…¥æ‚¨çš„é—®é¢˜",
        placeholder="ä¾‹å¦‚ï¼šè¿™ä¸ªç ”ç©¶é¢†åŸŸçš„ä¸»è¦ç ”ç©¶æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
        height=100
    )
    
    if question and st.button("èŽ·å–AIå›žç­”", type="primary"):
        try:
            with st.spinner("AIæ­£åœ¨æ€è€ƒ..."):
                # å‡†å¤‡ä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨å‰50ç¯‡æ–‡çŒ®çš„æ‘˜è¦ï¼‰
                context_texts = data['abstract_text'].dropna().head(50).tolist()
                context_sources = data['article_title'].head(50).tolist()
                
                # æ‰§è¡ŒAIé—®ç­”
                answer = answer_question_with_llm(question, context_texts)
                
                # æ˜¾ç¤ºç»“æžœ
                fig = create_llm_qa_visualization(question, answer, context_sources)
                st.plotly_chart(fig, use_container_width=True)
                
        except Exception as e:
            st.error(f"AIé—®ç­”å¤±è´¥: {e}")
            st.info("è¯·ç¡®ä¿å·²é…ç½®LLM APIå¯†é’¥")

def show_topic_label_optimization(data: pd.DataFrame):
    """æ˜¾ç¤ºä¸»é¢˜æ ‡ç­¾ä¼˜åŒ–åŠŸèƒ½"""
    st.subheader("ðŸ·ï¸ ä¸»é¢˜æ ‡ç­¾ä¼˜åŒ–")
    
    if 'topic_name' not in data.columns:
        st.warning("è¯·å…ˆå®Œæˆä¸»é¢˜åˆ†æž")
        return
    
    # é€‰æ‹©è¦ä¼˜åŒ–çš„ä¸»é¢˜
    available_topics = data['topic_name'].unique()
    available_topics = [t for t in available_topics if t != "Unclassified"]
    
    if available_topics:
        selected_topic = st.selectbox("é€‰æ‹©è¦ä¼˜åŒ–çš„ä¸»é¢˜", available_topics)
        
        if st.button("ç”Ÿæˆä¼˜åŒ–æ ‡ç­¾", type="primary"):
            try:
                with st.spinner("AIæ­£åœ¨ç”Ÿæˆä¼˜åŒ–æ ‡ç­¾..."):
                    # èŽ·å–ä¸»é¢˜çš„å…³é”®è¯å’Œä»£è¡¨æ€§æ–‡æ¡£
                    topic_data = data[data['topic_name'] == selected_topic]
                    topic_texts = topic_data['processed_text'].tolist()
                    
                    # å‡†å¤‡å…³é”®è¯å’Œæ–‡æ¡£ç‰‡æ®µ
                    topic_keywords = [(selected_topic, 1.0)]  # ç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨ä¸»é¢˜åç§°ä½œä¸ºå…³é”®è¯
                    representative_docs_snippets = topic_texts[:5]  # å–å‰5ä¸ªæ–‡æ¡£ä½œä¸ºä»£è¡¨
                    
                    # åˆ›å»ºLLMå®¢æˆ·ç«¯
                    try:
                        from literature_analyzer.ai_models import LLMClient
                        llm_client = LLMClient()
                    except Exception as e:
                        st.error(f"æ— æ³•åˆ›å»ºLLMå®¢æˆ·ç«¯: {e}")
                        return
                    
                    # ç”Ÿæˆä¼˜åŒ–æ ‡ç­¾
                    optimized_label = generate_llm_topic_label(
                        topic_keywords, 
                        representative_docs_snippets, 
                        llm_client
                    )
                    
                    # æ˜¾ç¤ºå¯¹æ¯”ç»“æžœ
                    original_labels = [selected_topic]
                    llm_labels = [optimized_label]
                    topic_ids = [1]  # ç®€åŒ–å¤„ç†
                    
                    fig = create_topic_llm_label_comparison(original_labels, llm_labels, topic_ids)
                    st.plotly_chart(fig, use_container_width=True)
                    
            except Exception as e:
                st.error(f"æ ‡ç­¾ä¼˜åŒ–å¤±è´¥: {e}")
                st.info("è¯·ç¡®ä¿å·²é…ç½®LLM APIå¯†é’¥")
    else:
        st.warning("æ²¡æœ‰å¯ç”¨çš„ä¸»é¢˜")

def show_advanced_analysis(data: pd.DataFrame):
    """æ˜¾ç¤ºé«˜çº§åˆ†æžé¡µé¢"""
    st.subheader("ðŸ“Š é«˜çº§åˆ†æžåŠŸèƒ½")
    
    if data.empty:
        st.warning("æ²¡æœ‰å¯åˆ†æžçš„æ•°æ®")
        return
    
    # åˆ›å»ºé«˜çº§åˆ†æžå­æ ‡ç­¾é¡µ
    advanced_tabs = st.tabs([
        "ðŸ”‘ å…³é”®è¯åˆ†æž",
        "ðŸ•¸ï¸ å…±çŽ°ç½‘ç»œ",
        "ðŸ“Š æ¨¡åž‹æ¯”è¾ƒ",
        "ðŸ”„ æ—¶é—´æ¼”åŒ–",
        "ðŸŒ é«˜çº§è¯­ä¹‰ç©ºé—´"
    ])
    
    with advanced_tabs[0]:
        show_keyword_analysis(data)
    
    with advanced_tabs[1]:
        show_cooccurrence_network(data)
    
    with advanced_tabs[2]:
        show_model_comparison(data)
    
    with advanced_tabs[3]:
        show_temporal_evolution(data)
    
    with advanced_tabs[4]:
        show_advanced_semantic_space(data)

def show_keyword_analysis(data: pd.DataFrame):
    """æ˜¾ç¤ºå…³é”®è¯åˆ†æžåŠŸèƒ½"""
    st.subheader("ðŸ”‘ é«˜çº§å…³é”®è¯åˆ†æž")
    
    # å…³é”®è¯æå–æ–¹æ³•é€‰æ‹©
    method = st.selectbox(
        "é€‰æ‹©å…³é”®è¯æå–æ–¹æ³•",
        ["YAKE", "KeyBERT", "TF-IDF"]
    )
    
    if st.button("æå–å…³é”®è¯", type="primary"):
        try:
            with st.spinner("æ­£åœ¨æå–å…³é”®è¯..."):
                # æå–å…³é”®è¯
                keywords = extract_keywords_advanced(
                    data['processed_text'].tolist(),
                    method=method
                )
                
                if keywords:
                    # åˆ›å»ºå…³é”®è¯æ ‘çŠ¶å›¾
                    fig = create_keyword_treemap(keywords)
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # æ˜¾ç¤ºå…³é”®è¯åˆ—è¡¨
                    st.subheader("ðŸ“‹ å…³é”®è¯åˆ—è¡¨")
                    keyword_df = pd.DataFrame(keywords, columns=['å…³é”®è¯', 'æƒé‡'])
                    st.dataframe(keyword_df.head(20), use_container_width=True)
                else:
                    st.warning("æœªèƒ½æå–åˆ°å…³é”®è¯")
                    
        except Exception as e:
            st.error(f"å…³é”®è¯æå–å¤±è´¥: {e}")

def show_cooccurrence_network(data: pd.DataFrame):
    """æ˜¾ç¤ºå…±çŽ°ç½‘ç»œåŠŸèƒ½"""
    st.subheader("ðŸ•¸ï¸ å®žä½“å…±çŽ°ç½‘ç»œ")
    
    # å®žä½“ç±»åž‹é€‰æ‹©
    entity_type = st.selectbox(
        "é€‰æ‹©å®žä½“ç±»åž‹",
        ["ä¸»é¢˜", "æœŸåˆŠ", "å¹´ä»½"]
    )
    
    # æœ€å°å…±çŽ°æ¬¡æ•°
    min_cooccurrence = st.slider(
        "æœ€å°å…±çŽ°æ¬¡æ•°",
        min_value=1,
        max_value=10,
        value=2
    )
    
    if st.button("ç”Ÿæˆå…±çŽ°ç½‘ç»œ", type="primary"):
        try:
            with st.spinner("æ­£åœ¨æž„å»ºå…±çŽ°ç½‘ç»œ..."):
                # æž„å»ºå…±çŽ°çŸ©é˜µ
                if entity_type == "ä¸»é¢˜":
                    cooccurrence_matrix = build_cooccurrence_matrix(
                        data['topic_name'].tolist(), min_cooccurrence
                    )
                elif entity_type == "æœŸåˆŠ":
                    cooccurrence_matrix = build_cooccurrence_matrix(
                        data['journal_title'].tolist(), min_cooccurrence
                    )
                else:  # å¹´ä»½
                    cooccurrence_matrix = build_cooccurrence_matrix(
                        data['publication_year'].astype(str).tolist(), min_cooccurrence
                    )
                
                # ç”Ÿæˆäº¤äº’å¼ç½‘ç»œå›¾
                network_html = create_cooccurrence_network(
                    cooccurrence_matrix, min_weight=min_cooccurrence
                )
                
                if network_html:
                    st.components.v1.html(network_html, height=600)
                else:
                    st.warning("æ— æ³•ç”Ÿæˆå…±çŽ°ç½‘ç»œ")
                    
        except Exception as e:
            st.error(f"å…±çŽ°ç½‘ç»œç”Ÿæˆå¤±è´¥: {e}")

def show_model_comparison(data: pd.DataFrame):
    """æ˜¾ç¤ºæ¨¡åž‹æ¯”è¾ƒåŠŸèƒ½"""
    st.subheader("ðŸ“Š ä¸»é¢˜æ¨¡åž‹æ¯”è¾ƒ")
    
    if st.session_state.embeddings is None:
        st.warning("è¯·å…ˆå®ŒæˆåµŒå…¥åˆ†æž")
        return
    
    # é€‰æ‹©è¦æ¯”è¾ƒçš„æ¨¡åž‹
    models_to_compare = st.multiselect(
        "é€‰æ‹©è¦æ¯”è¾ƒçš„ä¸»é¢˜æ¨¡åž‹",
        ["BERTopic", "LDA", "NMF", "KMeans"],
        default=["BERTopic", "LDA"]
    )
    
    if len(models_to_compare) >= 2 and st.button("æ‰§è¡Œæ¨¡åž‹æ¯”è¾ƒ", type="primary"):
        try:
            with st.spinner("æ­£åœ¨æ¯”è¾ƒä¸»é¢˜æ¨¡åž‹..."):
                # æ‰§è¡Œæ¨¡åž‹æ¯”è¾ƒ
                comparison_results = compare_topic_models(
                    data['processed_text'].tolist(),
                    st.session_state.embeddings,
                    models_to_compare
                )
                
                if comparison_results:
                    # åˆ›å»ºæ¨¡åž‹æ¯”è¾ƒå¯è§†åŒ–
                    fig = create_topic_model_comparison(comparison_results)
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # æ˜¾ç¤ºè¯¦ç»†æ¯”è¾ƒç»“æžœ
                    st.subheader("ðŸ“‹ æ¨¡åž‹æ€§èƒ½è¯¦æƒ…")
                    results_df = pd.DataFrame(comparison_results)
                    st.dataframe(results_df, use_container_width=True)
                else:
                    st.warning("æ¨¡åž‹æ¯”è¾ƒå¤±è´¥")
                    
        except Exception as e:
            st.error(f"æ¨¡åž‹æ¯”è¾ƒå¤±è´¥: {e}")

def show_temporal_evolution(data: pd.DataFrame):
    """æ˜¾ç¤ºæ—¶é—´æ¼”åŒ–åˆ†æžåŠŸèƒ½"""
    st.subheader("ðŸ”„ æ—¶é—´æ¼”åŒ–åˆ†æž")
    
    if 'topic_name' not in data.columns:
        st.warning("è¯·å…ˆå®Œæˆä¸»é¢˜åˆ†æž")
        return
    
    # é€‰æ‹©åˆ†æžç±»åž‹
    analysis_type = st.selectbox(
        "é€‰æ‹©åˆ†æžç±»åž‹",
        ["ä¸»é¢˜æ¼”åŒ–", "æœŸåˆŠæ¼”åŒ–", "å…³é”®è¯æ¼”åŒ–"]
    )
    
    if st.button("ç”Ÿæˆæ¼”åŒ–åˆ†æž", type="primary"):
        try:
            with st.spinner("æ­£åœ¨åˆ†æžæ—¶é—´æ¼”åŒ–..."):
                if analysis_type == "ä¸»é¢˜æ¼”åŒ–":
                    # è®¡ç®—ä¸»é¢˜çš„æ—¶é—´åˆ†å¸ƒ
                    temporal_data = data.groupby(['publication_year', 'topic_name']).size().reset_index(name='count')
                    temporal_data['proportion'] = temporal_data.groupby('publication_year')['count'].transform(lambda x: x / x.sum())
                    
                    fig = create_temporal_evolution_heatmap(
                        temporal_data, 'topic_name', 'publication_year', 'proportion'
                    )
                    st.plotly_chart(fig, use_container_width=True)
                    
                elif analysis_type == "æœŸåˆŠæ¼”åŒ–":
                    # è®¡ç®—æœŸåˆŠçš„æ—¶é—´åˆ†å¸ƒ
                    temporal_data = data.groupby(['publication_year', 'journal_title']).size().reset_index(name='count')
                    temporal_data['proportion'] = temporal_data.groupby('publication_year')['count'].transform(lambda x: x / x.sum())
                    
                    fig = create_temporal_evolution_heatmap(
                        temporal_data, 'journal_title', 'publication_year', 'proportion'
                    )
                    st.plotly_chart(fig, use_container_width=True)
                    
                else:  # å…³é”®è¯æ¼”åŒ–
                    st.info("å…³é”®è¯æ¼”åŒ–åˆ†æžéœ€è¦å…ˆæå–å…³é”®è¯")
                    
        except Exception as e:
            st.error(f"æ—¶é—´æ¼”åŒ–åˆ†æžå¤±è´¥: {e}")

def show_advanced_semantic_space(data: pd.DataFrame):
    """æ˜¾ç¤ºé«˜çº§è¯­ä¹‰ç©ºé—´åˆ†æžåŠŸèƒ½"""
    st.subheader("ðŸŒ é«˜çº§è¯­ä¹‰ç©ºé—´åˆ†æž")
    
    if st.session_state.embeddings is None:
        st.warning("è¯·å…ˆå®ŒæˆåµŒå…¥åˆ†æž")
        return
    
    # åˆ›å»ºé«˜çº§è¯­ä¹‰ç©ºé—´åˆ†æžå­æ ‡ç­¾é¡µ
    semantic_tabs = st.tabs([
        "ðŸ“Š å¤šç»´åº¦é™ç»´å¯¹æ¯”",
        "ðŸŒ äº¤äº’å¼3Dè¯­ä¹‰ç©ºé—´",
        "ðŸ•¸ï¸ è¯­ä¹‰ç›¸ä¼¼æ€§ç½‘ç»œ",
        "ðŸ“ˆ è¯­ä¹‰æ¼”åŒ–åˆ†æž",
        "ðŸ”¥ è¯­ä¹‰å¯†åº¦åˆ†æž",
        "ðŸŽ¯ è¯­ä¹‰èšç±»åˆ†æž",
        "ðŸ“Š è¯­ä¹‰æ¼‚ç§»åˆ†æž"
    ])
    
    with semantic_tabs[0]:
        show_multi_dimensional_reduction(data)
    
    with semantic_tabs[1]:
        show_interactive_3d_semantic_space(data)
    
    with semantic_tabs[2]:
        show_semantic_similarity_network(data)
    
    with semantic_tabs[3]:
        show_semantic_evolution_analysis(data)
    
    with semantic_tabs[4]:
        show_semantic_density_analysis(data)
    
    with semantic_tabs[5]:
        show_semantic_clustering_analysis(data)
    
    with semantic_tabs[6]:
        show_semantic_drift_analysis(data)

def show_multi_dimensional_reduction(data: pd.DataFrame):
    """æ˜¾ç¤ºå¤šç»´åº¦é™ç»´å¯¹æ¯”åŠŸèƒ½"""
    st.subheader("ðŸ“Š å¤šç»´åº¦é™ç»´æ–¹æ³•å¯¹æ¯”")
    
    # é€‰æ‹©è¦å¯¹æ¯”çš„é™ç»´æ–¹æ³•
    reduction_methods = st.multiselect(
        "é€‰æ‹©é™ç»´æ–¹æ³•",
        ["UMAP", "t-SNE", "PCA"],
        default=["UMAP", "t-SNE"]
    )
    
    if len(reduction_methods) >= 2 and st.button("ç”Ÿæˆé™ç»´å¯¹æ¯”", type="primary"):
        try:
            with st.spinner("æ­£åœ¨æ‰§è¡Œé™ç»´å¯¹æ¯”..."):
                # èŽ·å–åµŒå…¥å‘é‡å’Œæ ‡ç­¾
                embeddings = st.session_state.embeddings
                labels = data['article_title'].tolist()
                
                # é¢œè‰²æ˜ å°„
                color_options = []
                if 'topic_name' in data.columns:
                    color_options.append('topic_name')
                if 'journal_title' in data.columns:
                    color_options.append('journal_title')
                
                colors = None
                if color_options:
                    selected_color = st.selectbox("é€‰æ‹©é¢œè‰²æ˜ å°„", color_options)
                    colors = data[selected_color].tolist()
                
                # ç”Ÿæˆå¯¹æ¯”å›¾
                fig = create_multi_dimensional_semantic_space(
                    embeddings,
                    labels=labels,
                    colors=colors,
                    methods=reduction_methods
                )
                st.plotly_chart(fig, use_container_width=True)
                
        except Exception as e:
            st.error(f"é™ç»´å¯¹æ¯”å¤±è´¥: {e}")

def show_interactive_3d_semantic_space(data: pd.DataFrame):
    """æ˜¾ç¤ºäº¤äº’å¼3Dè¯­ä¹‰ç©ºé—´åŠŸèƒ½"""
    st.subheader("ðŸŒ äº¤äº’å¼3Dè¯­ä¹‰ç©ºé—´")
    
    # é€‰æ‹©é™ç»´æ–¹æ³•
    reduction_method = st.selectbox(
        "é€‰æ‹©é™ç»´æ–¹æ³•",
        ["UMAP", "t-SNE", "PCA"],
        index=0
    )
    
    # é¢œè‰²æ˜ å°„é€‰æ‹©
    color_options = []
    if 'topic_name' in data.columns:
        color_options.append('topic_name')
    if 'journal_title' in data.columns:
        color_options.append('journal_title')
    if 'publication_year' in data.columns:
        color_options.append('publication_year')
    
    if color_options:
        selected_color = st.selectbox("é€‰æ‹©é¢œè‰²æ˜ å°„", color_options)
        
        if st.button("ç”Ÿæˆäº¤äº’å¼3Dç©ºé—´", type="primary"):
            try:
                with st.spinner("æ­£åœ¨ç”Ÿæˆäº¤äº’å¼3Dè¯­ä¹‰ç©ºé—´..."):
                    # èŽ·å–åµŒå…¥å‘é‡å’Œæ ‡ç­¾
                    embeddings = st.session_state.embeddings
                    labels = data['article_title'].tolist()
                    colors = data[selected_color].tolist()
                    
                    # ç”Ÿæˆäº¤äº’å¼3Då›¾
                    fig = create_semantic_space_3d_interactive(
                        embeddings,
                        labels=labels,
                        colors=colors,
                        method=reduction_method
                    )
                    st.plotly_chart(fig, use_container_width=True)
                    
            except Exception as e:
                st.error(f"äº¤äº’å¼3Dç©ºé—´ç”Ÿæˆå¤±è´¥: {e}")
    else:
        st.warning("æ²¡æœ‰å¯ç”¨çš„é¢œè‰²æ˜ å°„é€‰é¡¹")

def show_semantic_similarity_network(data: pd.DataFrame):
    """æ˜¾ç¤ºè¯­ä¹‰ç›¸ä¼¼æ€§ç½‘ç»œåŠŸèƒ½"""
    st.subheader("ðŸ•¸ï¸ è¯­ä¹‰ç›¸ä¼¼æ€§ç½‘ç»œ")
    
    # ç›¸ä¼¼æ€§é˜ˆå€¼
    similarity_threshold = st.slider(
        "ç›¸ä¼¼æ€§é˜ˆå€¼",
        min_value=0.1,
        max_value=0.9,
        value=0.6,
        step=0.05,
        help="æŽ§åˆ¶ç½‘ç»œè¾¹çš„å¯†åº¦ï¼Œå€¼è¶Šé«˜ç½‘ç»œè¶Šç¨€ç–"
    )
    
    # æœ€å¤§èŠ‚ç‚¹æ•°
    max_nodes = st.slider(
        "æœ€å¤§èŠ‚ç‚¹æ•°",
        min_value=10,
        max_value=200,
        value=50,
        help="é™åˆ¶ç½‘ç»œä¸­æ˜¾ç¤ºçš„æœ€å¤§èŠ‚ç‚¹æ•°"
    )
    
    if st.button("ç”Ÿæˆè¯­ä¹‰ç›¸ä¼¼æ€§ç½‘ç»œ", type="primary"):
        try:
            with st.spinner("æ­£åœ¨ç”Ÿæˆè¯­ä¹‰ç›¸ä¼¼æ€§ç½‘ç»œ..."):
                # åˆ›å»ºè¯­ä¹‰ç©ºé—´åˆ†æžå™¨
                analyzer = SemanticSpaceAnalyzer(
                    st.session_state.embeddings,
                    data['article_title'].tolist()
                )
                
                # ç”Ÿæˆç½‘ç»œå›¾
                network_html = analyzer.create_semantic_network(
                    threshold=similarity_threshold,
                    max_nodes=max_nodes
                )
                
                if network_html:
                    st.components.v1.html(network_html, height=600)
                    
                    # ä¸‹è½½ç½‘ç»œå›¾
                    st.download_button(
                        label="ðŸ“¥ ä¸‹è½½ç½‘ç»œå›¾ (HTML)",
                        data=network_html,
                        file_name='semantic_similarity_network.html',
                        mime='text/html'
                    )
                else:
                    st.warning("æ— æ³•ç”Ÿæˆè¯­ä¹‰ç›¸ä¼¼æ€§ç½‘ç»œ")
                    
        except Exception as e:
            st.error(f"è¯­ä¹‰ç›¸ä¼¼æ€§ç½‘ç»œç”Ÿæˆå¤±è´¥: {e}")

def show_semantic_evolution_analysis(data: pd.DataFrame):
    """æ˜¾ç¤ºè¯­ä¹‰æ¼”åŒ–åˆ†æžåŠŸèƒ½"""
    st.subheader("ðŸ“ˆ è¯­ä¹‰æ¼”åŒ–åˆ†æž")
    
    if 'publication_year' not in data.columns:
        st.warning("æ•°æ®ä¸­ç¼ºå°‘å¹´ä»½ä¿¡æ¯ï¼Œæ— æ³•è¿›è¡Œæ¼”åŒ–åˆ†æž")
        return
    
    # é€‰æ‹©æ—¶é—´æ­¥é•¿
    time_step_options = ["æ¯å¹´", "æ¯2å¹´", "æ¯3å¹´", "æ¯5å¹´"]
    selected_time_step = st.selectbox("é€‰æ‹©æ—¶é—´æ­¥é•¿", time_step_options)
    
    # èŽ·å–å¹´ä»½èŒƒå›´
    year_min, year_max = data['publication_year'].min(), data['publication_year'].max()
    
    if st.button("ç”Ÿæˆè¯­ä¹‰æ¼”åŒ–åˆ†æž", type="primary"):
        try:
            with st.spinner("æ­£åœ¨åˆ†æžè¯­ä¹‰æ¼”åŒ–..."):
                # åˆ›å»ºæ—¶é—´åºåˆ—åˆ†æžå™¨
                temporal_analyzer = TemporalSemanticAnalyzer()
                
                # æ ¹æ®æ—¶é—´æ­¥é•¿åˆ†ç»„æ•°æ®
                step_mapping = {
                    "æ¯å¹´": 1,
                    "æ¯2å¹´": 2,
                    "æ¯3å¹´": 3,
                    "æ¯5å¹´": 5
                }
                step = step_mapping[selected_time_step]
                
                # åˆ†ç»„å¹¶æ·»åŠ æ—¶é—´æ­¥æ•°æ®
                for year in range(year_min, year_max + 1, step):
                    year_data = data[
                        (data['publication_year'] >= year) & 
                        (data['publication_year'] < year + step)
                    ]
                    
                    if not year_data.empty:
                        # èŽ·å–å¯¹åº”çš„åµŒå…¥å‘é‡
                        year_indices = year_data.index.tolist()
                        year_embeddings = st.session_state.embeddings[year_indices]
                        year_titles = year_data['article_title'].tolist()
                        
                        temporal_analyzer.add_time_step(
                            f"{year}-{year+step-1}",
                            year_embeddings,
                            year_titles
                        )
                
                # åˆ†æžæ¼”åŒ–
                evolution_results = temporal_analyzer.analyze_temporal_evolution()
                
                # æ˜¾ç¤ºæ¼”åŒ–æŒ‡æ ‡
                st.subheader("ðŸ“Š æ¼”åŒ–æŒ‡æ ‡")
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("å¹³å‡æ¼‚ç§»", f"{evolution_results['stability_metrics']['mean_drift']:.3f}")
                with col2:
                    st.metric("æ¼‚ç§»è¶‹åŠ¿", evolution_results['stability_metrics']['drift_trend'])
                
                # ç”Ÿæˆæ¼”åŒ–åŠ¨ç”»
                animation_fig = temporal_analyzer.create_evolution_animation()
                st.plotly_chart(animation_fig, use_container_width=True)
                
        except Exception as e:
            st.error(f"è¯­ä¹‰æ¼”åŒ–åˆ†æžå¤±è´¥: {e}")

def show_semantic_density_analysis(data: pd.DataFrame):
    """æ˜¾ç¤ºè¯­ä¹‰å¯†åº¦åˆ†æžåŠŸèƒ½"""
    st.subheader("ðŸ”¥ è¯­ä¹‰å¯†åº¦åˆ†æž")
    
    # ç½‘æ ¼å¤§å°
    grid_size = st.slider(
        "ç½‘æ ¼å¤§å°",
        min_value=10,
        max_value=100,
        value=30,
        help="æŽ§åˆ¶å¯†åº¦çƒ­åŠ›å›¾çš„ç²¾åº¦ï¼Œå€¼è¶Šå¤§ç²¾åº¦è¶Šé«˜"
    )
    
    # é™ç»´æ–¹æ³•é€‰æ‹©
    reduction_method = st.selectbox(
        "é€‰æ‹©é™ç»´æ–¹æ³•",
        ["UMAP", "t-SNE", "PCA"],
        index=0,
        key="reduction_method_3d"
    )
    
    if st.button("ç”Ÿæˆè¯­ä¹‰å¯†åº¦çƒ­åŠ›å›¾", type="primary"):
        try:
            with st.spinner("æ­£åœ¨ç”Ÿæˆè¯­ä¹‰å¯†åº¦çƒ­åŠ›å›¾..."):
                # åˆ›å»ºè¯­ä¹‰ç©ºé—´åˆ†æžå™¨
                analyzer = SemanticSpaceAnalyzer(
                    st.session_state.embeddings,
                    data['article_title'].tolist()
                )
                
                # ç”Ÿæˆå¯†åº¦çƒ­åŠ›å›¾
                density_fig = analyzer.create_density_heatmap(grid_size=grid_size)
                st.plotly_chart(density_fig, use_container_width=True)
                
                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                stats = analyzer.get_semantic_statistics()
                st.subheader("ðŸ“Š è¯­ä¹‰ç©ºé—´ç»Ÿè®¡")
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("åµŒå…¥ç»´åº¦", stats['embedding_dimension'])
                    st.metric("æ–‡æ¡£æ•°é‡", stats['num_documents'])
                with col2:
                    st.metric("å¹³å‡ç›¸ä¼¼åº¦", f"{stats['mean_similarity']:.3f}")
                    st.metric("ç›¸ä¼¼åº¦æ ‡å‡†å·®", f"{stats['std_similarity']:.3f}")
                
        except Exception as e:
            st.error(f"è¯­ä¹‰å¯†åº¦åˆ†æžå¤±è´¥: {e}")

def show_semantic_clustering_analysis(data: pd.DataFrame):
    """æ˜¾ç¤ºè¯­ä¹‰èšç±»åˆ†æžåŠŸèƒ½"""
    st.subheader("ðŸŽ¯ è¯­ä¹‰èšç±»åˆ†æž")
    
    # èšç±»æ–¹æ³•é€‰æ‹©
    clustering_method = st.selectbox(
        "é€‰æ‹©èšç±»æ–¹æ³•",
        ["KMeans", "DBSCAN", "Agglomerative"],
        index=0,
        key="clustering_method_semantic"
    )
    
    # èšç±»æ•°é‡ï¼ˆå¯¹äºŽéœ€è¦æŒ‡å®šæ•°é‡çš„æ–¹æ³•ï¼‰
    if clustering_method in ["KMeans", "Agglomerative"]:
        n_clusters = st.slider(
            "èšç±»æ•°é‡",
            min_value=2,
            max_value=20,
            value=5
        )
    
    # DBSCANå‚æ•°
    if clustering_method == "DBSCAN":
        eps = st.slider(
            "epså‚æ•°",
            min_value=0.1,
            max_value=2.0,
            value=0.5,
            step=0.1
        )
        min_samples = st.slider(
            "min_sampleså‚æ•°",
            min_value=1,
            max_value=10,
            value=2
        )
    
    if st.button("æ‰§è¡Œè¯­ä¹‰èšç±»åˆ†æž", type="primary"):
        try:
            with st.spinner("æ­£åœ¨æ‰§è¡Œè¯­ä¹‰èšç±»åˆ†æž..."):
                # åˆ›å»ºè¯­ä¹‰ç©ºé—´åˆ†æžå™¨
                analyzer = SemanticSpaceAnalyzer(
                    st.session_state.embeddings,
                    data['article_title'].tolist()
                )
                
                # æ‰§è¡Œèšç±»
                if clustering_method == "KMeans":
                    cluster_labels = analyzer.perform_clustering(
                        method="KMeans",
                        n_clusters=n_clusters
                    )
                elif clustering_method == "DBSCAN":
                    cluster_labels = analyzer.perform_clustering(
                        method="DBSCAN",
                        eps=eps,
                        min_samples=min_samples
                    )
                else:  # Agglomerative
                    cluster_labels = analyzer.perform_clustering(
                        method="Agglomerative",
                        n_clusters=n_clusters
                    )
                
                # ç”Ÿæˆèšç±»åˆ†æžå›¾
                fig = create_semantic_clustering_analysis(
                    st.session_state.embeddings,
                    labels=data['article_title'].tolist(),
                    n_clusters=len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
                )
                st.plotly_chart(fig, use_container_width=True)
                
                # æ˜¾ç¤ºèšç±»ç»Ÿè®¡
                unique_labels = set(cluster_labels)
                n_clusters_found = len(unique_labels) - (1 if -1 in unique_labels else 0)
                st.metric("å‘çŽ°çš„èšç±»æ•°", n_clusters_found)
                
                # å¦‚æžœæœ‰è¶³å¤Ÿçš„èšç±»ï¼Œæ˜¾ç¤ºè½®å»“ç³»æ•°
                if n_clusters_found > 1:
                    from sklearn.metrics import silhouette_score
                    silhouette = silhouette_score(st.session_state.embeddings, cluster_labels)
                    st.metric("è½®å»“ç³»æ•°", f"{silhouette:.3f}")
                
        except Exception as e:
            st.error(f"è¯­ä¹‰èšç±»åˆ†æžå¤±è´¥: {e}")

def show_semantic_drift_analysis(data: pd.DataFrame):
    """æ˜¾ç¤ºè¯­ä¹‰æ¼‚ç§»åˆ†æžåŠŸèƒ½"""
    st.subheader("ðŸ“Š è¯­ä¹‰æ¼‚ç§»åˆ†æž")
    
    if 'publication_year' not in data.columns:
        st.warning("æ•°æ®ä¸­ç¼ºå°‘å¹´ä»½ä¿¡æ¯ï¼Œæ— æ³•è¿›è¡Œæ¼‚ç§»åˆ†æž")
        return
    
    # é€‰æ‹©å¯¹æ¯”çš„æ—¶é—´æ®µ
    years = sorted(data['publication_year'].unique())
    if len(years) < 2:
        st.warning("æ•°æ®ä¸­å¹´ä»½ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œæ¼‚ç§»åˆ†æž")
        return
    
    # æ—¶é—´æ®µé€‰æ‹©
    col1, col2 = st.columns(2)
    with col1:
        year1 = st.selectbox("é€‰æ‹©ç¬¬ä¸€ä¸ªå¹´ä»½", years[:-1])
    with col2:
        year2 = st.selectbox("é€‰æ‹©ç¬¬äºŒä¸ªå¹´ä»½", years[1:])
    
    # æ¼‚ç§»åˆ†æžæ–¹æ³•
    drift_method = st.selectbox(
        "é€‰æ‹©æ¼‚ç§»åˆ†æžæ–¹æ³•",
        ["è´¨å¿ƒæ¼‚ç§»", "åˆ†å¸ƒæ¼‚ç§»", "æˆå¯¹è·ç¦»"],
        index=0,
        key="drift_method_semantic"
    )
    
    if st.button("æ‰§è¡Œè¯­ä¹‰æ¼‚ç§»åˆ†æž", type="primary"):
        try:
            with st.spinner("æ­£åœ¨åˆ†æžè¯­ä¹‰æ¼‚ç§»..."):
                # èŽ·å–ä¸¤ä¸ªæ—¶é—´æ®µçš„åµŒå…¥å‘é‡
                embeddings_1 = st.session_state.embeddings[data['publication_year'] == year1]
                embeddings_2 = st.session_state.embeddings[data['publication_year'] == year2]
                
                # åˆ›å»ºåˆ†æžå™¨å¹¶åˆ†æžæ¼‚ç§»
                analyzer = SemanticSpaceAnalyzer(embeddings_1)
                
                method_mapping = {
                    "è´¨å¿ƒæ¼‚ç§»": "centroid",
                    "åˆ†å¸ƒæ¼‚ç§»": "distribution",
                    "æˆå¯¹è·ç¦»": "pairwise"
                }
                
                drift_metrics = analyzer.analyze_semantic_drift(
                    embeddings_2,
                    method=method_mapping[drift_method]
                )
                
                # æ˜¾ç¤ºæ¼‚ç§»æŒ‡æ ‡
                st.subheader("ðŸ“Š æ¼‚ç§»åˆ†æžç»“æžœ")
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("æ¼‚ç§»è·ç¦»", f"{drift_metrics['drift_distance']:.3f}")
                with col2:
                    st.metric("ç›¸å¯¹æ¼‚ç§»", f"{drift_metrics['relative_drift']:.3f}")
                with col3:
                    st.metric("æ¼‚ç§»æ–¹å‘", drift_metrics['drift_direction'])
                
                # ç”Ÿæˆæ¼‚ç§»å¯¹æ¯”å›¾
                embeddings_dict = {
                    f"{year1}å¹´": embeddings_1,
                    f"{year2}å¹´": embeddings_2
                }
                
                drift_fig = create_semantic_drift_comparison(
                    embeddings_dict, 
                    reference_key=f"{year1}å¹´"
                )
                st.plotly_chart(drift_fig, use_container_width=True)
                
        except Exception as e:
            st.error(f"è¯­ä¹‰æ¼‚ç§»åˆ†æžå¤±è´¥: {e}")

if __name__ == "__main__":
    nlp = download_spacy_model(MODEL_NAME)
    main()
